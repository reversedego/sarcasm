{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_set = train_data.dropna() # Drop any rows with anything missing. Not too many of them anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '5d', 'actually less', 'all about', 'am proud', 'and brian', 'and pillow', 'any', 'are personally', 'asia and', 'babies we', 'be drawn', 'because your', 'bettas', 'bogan', 'buddy pal', 'by modern', 'card art', 'check yourself', 'clothes for', 'condolences on', 'couldn stay', 'cyrillic', 'definition of', 'different job', 'dollar tree', 'dsr', 'embody', 'eventually they', 'f6', 'fever is', 'folded', 'forced out', 'fuck yo', 'generalising', 'go badly', 'gowdy', 'had different', 'hates everyone', 'he ruins', 'hide then', 'holiday spirit', 'iconic', 'in even', 'init', 'is did', 'iso', 'its shitty', 'just last', 'kiting', 'learn your', 'like holding', 'lol great', 'main character', 'matt cassel', 'met his', 'money would', 'muddled', 'names that', 'nice stuff', 'not half', 'obvs', 'of personal', 'olympian', 'only end', 'orioles', 'page says', 'people react', 'planted by', 'positives', 'probably nothing', 'quad', 're oppressing', 'reddits', 'reveals the', 'running off', 'school days', 'series as', 'short is', 'skins to', 'softer', 'spaz', 'steam store', 'sub needs', 'sympathizing', 'term abortion', 'that justifies', 'the beans', 'the frontpage', 'the outback', 'the too', 'then fix', 'think its', 'threaten his', 'to discuss', 'to vt', 'traps', 'ui is', 'us over', 'vile', 'was joke', 'week end', 'when everybody', 'will blame', 'with your', 'would sure', 'yes most', 'your coworker']\n",
      "215441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  0.7215785423766077\n",
      "(682272, 215441)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Model with using empirically guessed parameters and methods\n",
    "no stopwords,\n",
    "tfidf as vectoriser,\n",
    "min_df = 5,\n",
    "ngram_range = (1,2)\n",
    "\n",
    "\n",
    "Found sources and examples online with more advanced techniques like stemming / lemmatisation on \n",
    "this particular dataset but decided against them since they dont improve accuracy by much.\n",
    "\n",
    "It would make sense to add the subreddit as one-hotted features, weekend as a binary numerical feature, \n",
    "along with score, upvotes and downvotes as continuous numerical features but dont have enough time\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "vect = TfidfVectorizer(min_df=5,ngram_range=(1,2))\n",
    "vect.fit(test_set['comment'])\n",
    "\n",
    "\n",
    "X_tr, X_tst, y_train, y_test = train_test_split(test_set['comment'],test_set['label'])\n",
    "\n",
    "X_train = vect.transform(X_tr)\n",
    "X_test = vect.transform(X_tst)\n",
    "\n",
    "feature_names = vect.get_feature_names()\n",
    "print(feature_names[::2000])\n",
    "print(len(feature_names))\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "score = clf.score(X_test,y_test)\n",
    "print(\"score: \", score)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(909697, 215441)\n",
      "['00', '5d', 'actually less', 'all about', 'am proud', 'and brian', 'and pillow', 'any', 'are personally', 'asia and', 'babies we', 'be drawn', 'because your', 'bettas', 'bogan', 'buddy pal', 'by modern', 'card art', 'check yourself', 'clothes for', 'condolences on', 'couldn stay', 'cyrillic', 'definition of', 'different job', 'dollar tree', 'dsr', 'embody', 'eventually they', 'f6', 'fever is', 'folded', 'forced out', 'fuck yo', 'generalising', 'go badly', 'gowdy', 'had different', 'hates everyone', 'he ruins', 'hide then', 'holiday spirit', 'iconic', 'in even', 'init', 'is did', 'iso', 'its shitty', 'just last', 'kiting', 'learn your', 'like holding', 'lol great', 'main character', 'matt cassel', 'met his', 'money would', 'muddled', 'names that', 'nice stuff', 'not half', 'obvs', 'of personal', 'olympian', 'only end', 'orioles', 'page says', 'people react', 'planted by', 'positives', 'probably nothing', 'quad', 're oppressing', 'reddits', 'reveals the', 'running off', 'school days', 'series as', 'short is', 'skins to', 'softer', 'spaz', 'steam store', 'sub needs', 'sympathizing', 'term abortion', 'that justifies', 'the beans', 'the frontpage', 'the outback', 'the too', 'then fix', 'think its', 'threaten his', 'to discuss', 'to vt', 'traps', 'ui is', 'us over', 'vile', 'was joke', 'week end', 'when everybody', 'will blame', 'with your', 'would sure', 'yes most', 'your coworker']\n",
      "215441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming the model is stable. \n",
    "# That is reasonable since we didnt have notable spikes or drops\n",
    "# in accuracy during many random samplings building train/test sets.\n",
    "# We can now train on the full data set and go for validation\n",
    "\n",
    "X = vect.transform(test_set['comment'])\n",
    "print(X.shape)\n",
    "y = test_set['label']\n",
    "\n",
    "feature_names = vect.get_feature_names()\n",
    "print(feature_names[::2000])\n",
    "print(len(feature_names))\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Now to get the list of id - label from the test.csv\n",
    "raw_validation_set = pd.read_csv('data/test.csv')\n",
    "validation_set = raw_validation_set.dropna()\n",
    "validation_transformed = vect.transform(validation_set['comment'])\n",
    "validation_set['predicted_label'] = clf.predict(validation_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(raw_validation_set.shape)\n",
    "# print(validation_set.shape)\n",
    "# validation_set['id'].value_counts()\n",
    "# raw_validation_set['id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are nan values in the original csv, \n",
    "# we have to fix them by joining to get the right labels on the right rows instead of just outputting the column\n",
    "output_set = raw_validation_set.merge(validation_set, how='left', left_on='id', right_on='id').filter(['id', 'predicted_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns, create the resulting csv\n",
    "validation_set = validation_set.filter(['predicted_label'])\n",
    "output_set.to_csv('results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
